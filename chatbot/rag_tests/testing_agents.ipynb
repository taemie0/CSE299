{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RAG Pipeline Test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'E:\\CSE299\\chatbot')\n",
    "from query_Prompt import query_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# Custom conversation template for the assistant\n",
    "CUSTOM_CONVERSATION_TEMPLATE = \"\"\"\n",
    "The following is a conversation with a helpful assistant. The assistant is intelligent, knowledgeable, and tries to help the user in the best possible way.\n",
    "\n",
    "Current Conversation:\n",
    "{history}\n",
    "\n",
    "Tools available:\n",
    "{tools}\n",
    "\n",
    "Tool names:\n",
    "{tool_names}\n",
    "\n",
    "Agent Scratchpad:\n",
    "{agent_scratchpad}\n",
    "\n",
    "User: {input}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# Tool 1: Physics Knowledge from RAG (retrieval using ChromaDB)\n",
    "def fetch_from_rag(query: str):\n",
    "    return query_rag(query)\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"Physics Retrieval\",\n",
    "    func=fetch_from_rag,\n",
    "    description=\"Fetch physics-related information from RAG using the local vector database.\"\n",
    ")\n",
    "\n",
    "# Tool 2: Calculation Tool (Simple calculator using Python's eval)\n",
    "def physics_calculator(expression: str):\n",
    "    try:\n",
    "        return eval(expression)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "calc_tool = Tool(\n",
    "    name=\"Physics Calculator\",\n",
    "    func=physics_calculator,\n",
    "    description=\"Perform calculations relevant to physics problems.\"\n",
    ")\n",
    "\n",
    "# Tool 3: Conversation Memory Tool\n",
    "def conversation_memory(query: str, chat_history: list):\n",
    "    # Fetch previous memory for the session from MongoDB\n",
    "    # conversation_history = fetch_history(session_name)\n",
    "    return query_rag(query, chat_history) if chat_history else None\n",
    "\n",
    "memory_tool = Tool(\n",
    "    name=\"Conversation Memory\",\n",
    "    func=conversation_memory,\n",
    "    description=\"Hold conversation with memory.\"\n",
    ")\n",
    "\n",
    "# Initialize your LLM (Ollama, in this case)\n",
    "llm = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "# Create a list of tools to pass to the agent\n",
    "tools = [rag_tool, calc_tool, memory_tool]\n",
    "\n",
    "# Extract tool names for the prompt\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "# Use create_react_agent for initializing the agent\n",
    "agent = create_react_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    prompt=ChatPromptTemplate.from_template(CUSTOM_CONVERSATION_TEMPLATE),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'history', 'input'], input_types={}, partial_variables={'tools': 'Physics Retrieval(query: str) - Fetch physics-related information from RAG using the local vector database.\\nPhysics Calculator(expression: str) - Perform calculations relevant to physics problems.\\nConversation Memory(query: str, chat_history: list) - Hold conversation with memory.', 'tool_names': 'Physics Retrieval, Physics Calculator, Conversation Memory'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'history', 'input', 'tool_names', 'tools'], input_types={}, partial_variables={}, template='\\nThe following is a conversation with a helpful assistant. The assistant is intelligent, knowledgeable, and tries to help the user in the best possible way.\\n\\nCurrent Conversation:\\n{history}\\n\\nTools available:\\n{tools}\\n\\nTool names:\\n{tool_names}\\n\\nAgent Scratchpad:\\n{agent_scratchpad}\\n\\nUser: {input}\\n\\nAssistant:'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=Ollama(model='gemma2:2b'), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str, chat_history: list):\n",
    "    # Load previous memory into the agent's memory if available\n",
    "    # conversation_history = fetch_history(session_name)\n",
    "    \n",
    "    # Update the conversation history in the prompt\n",
    "    if chat_history:\n",
    "        formatted_history = \"\\n\".join(\n",
    "            f\"{entry['role']}: {entry['content']}\" for entry in chat_history\n",
    "        )\n",
    "    else:\n",
    "        formatted_history = \"\"\n",
    "\n",
    "    memory_input = f\"{formatted_history}\\nUser: {query}\\nAssistant:\"\n",
    "    # Prepare the agent scratchpad (you can customize this based on your logic)\n",
    "    agent_scratchpad = \"\"  # Initialize or load your scratchpad data if needed\n",
    "\n",
    "    # Run the agent with the user query\n",
    "    result = agent.invoke({\n",
    "        \"input\": query,\n",
    "        \"history\": memory_input,\n",
    "        \"tools\": \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "        \"tool_names\": \", \".join(tool_names),\n",
    "        \"agent_scratchpad\": agent_scratchpad,\n",
    "    })\n",
    "    print(result) \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample chat history\n",
    "chat_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is gravitational force?\"},\n",
    "    {\"role\": \"bot\", \"content\": \"Gravitational force is the force of attraction between two masses...\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_chatbot():\n",
    "\n",
    "    try:\n",
    "        # Simulate a user input\n",
    "        user_input = \"My name is Mehar.\"\n",
    "        \n",
    "        # Print current chat history for debugging\n",
    "        print(\"Current Chat History:\")\n",
    "        for entry in chat_history:\n",
    "            print(f\"{entry['role']}: {entry['content']}\")\n",
    "        \n",
    "        # Run the agent with user input and the current chat history\n",
    "        response = run_agent(user_input, chat_history)\n",
    "        \n",
    "        # Print the response from the agent\n",
    "        print(\"\\nResponse from the chatbot:\")\n",
    "        print(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print any exceptions that occur during the process\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Chat History:\n",
      "user: What is gravitational force?\n",
      "bot: Gravitational force is the force of attraction between two masses...\n",
      "An error occurred: 'intermediate_steps'\n"
     ]
    }
   ],
   "source": [
    "debug_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'history', 'input'], input_types={}, partial_variables={'tools': 'Physics Retrieval(query: str) - Fetch physics-related information from RAG using the local vector database.\\nPhysics Calculator(expression: str) - Perform calculations relevant to physics problems.\\nConversation Memory(query: str, chat_history: list) - Hold conversation with memory.', 'tool_names': 'Physics Retrieval, Physics Calculator, Conversation Memory'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'history', 'input', 'tool_names', 'tools'], input_types={}, partial_variables={}, template='\\nThe following is a conversation with a helpful assistant. The assistant is intelligent, knowledgeable, and tries to help the user in the best possible way.\\n\\nCurrent Conversation:\\n{history}\\n\\nTools available:\\n{tools}\\n\\nTool names:\\n{tool_names}\\n\\nAgent Scratchpad:\\n{agent_scratchpad}\\n\\nUser: {input}\\n\\nAssistant:'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=Ollama(model='gemma2:2b'), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import Tool, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.1-cp311-cp311-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in e:\\cse299\\chatbot\\venv\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.1-cp311-cp311-win_amd64.whl (141 kB)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numexpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_chain = LLMMathChain.from_llm(llm=Ollama(model=\"gemma2:2b\"))\n",
    "math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                func=problem_chain.run,\n",
    "                description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only inputmath expressions.\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_CONVERSATION_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that can perform reasoning and calculations. You are equipped with the following tools: {tool_names}. \n",
    "\n",
    "Current conversation:\n",
    "{agent_scratchpad}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_react_agent\n",
    "\n",
    "# Define the word problem prompt template\n",
    "word_problem_template = \"\"\"You are a reasoning agent tasked with solving \n",
    "the user's logic-based questions. Logically arrive at the solution, and be \n",
    "factual. In your answers, clearly detail the steps involved and give the \n",
    "final answer. Provide the response in bullet points. \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "# Define the prompt for the reasoning tool\n",
    "math_assistant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                       template=word_problem_template)\n",
    "\n",
    "# Create the reasoning chain (reasoning tool)\n",
    "word_problem_chain = LLMChain(llm=llm,\n",
    "                              prompt=math_assistant_prompt)\n",
    "\n",
    "# Define the reasoning tool (for logic-based questions)\n",
    "word_problem_tool = Tool.from_function(\n",
    "    name=\"Reasoning Tool\",\n",
    "    func=word_problem_chain.run,\n",
    "    description=\"Useful for when you need to answer logic-based/reasoning questions.\"\n",
    ")\n",
    "\n",
    "# Define the math tool (for arithmetic questions)\n",
    "math_tool = Tool.from_function(\n",
    "    name=\"Calculator\",\n",
    "    func=lambda expression: eval(expression),  # Simple calculator, use a safer eval in production\n",
    "    description=\"Useful for answering math questions.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from pymongo import MongoClient\n",
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationEntityMemory\nllm\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mConversationEntityMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\CSE299\\chatbot\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:213\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     emit_warning()\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\CSE299\\chatbot\\venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\CSE299\\chatbot\\venv\\Lib\\site-packages\\pydantic\\main.py:209\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    208\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    211\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    215\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ConversationEntityMemory\nllm\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing"
     ]
    }
   ],
   "source": [
    "ConversationEntityMemory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
