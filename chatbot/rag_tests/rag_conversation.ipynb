{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requiremnets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths in your notebook\n",
    "model_path = r\"E:/CSE299/chatbot/llm\"\n",
    "embedding_save_path = r\"E:/CSE299/chatbot/Embedding\"\n",
    "pdfs_path = r\"E:/CSE299/chatbot/docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# Function to stream output of the subprocess\n",
    "def stream_output(process):\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')  # Output is already a string, so no need to decode\n",
    "\n",
    "# Function to start Ollama\n",
    "def start_ollama():\n",
    "    try:\n",
    "        # Redirect stdout and stderr to os.devnull\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            ollama_process = subprocess.Popen(\n",
    "                [\"ollama\", \"serve\"], \n",
    "                stdout=devnull,  # Discard stdout\n",
    "                stderr=devnull,  # Discard stderr\n",
    "                shell=True       # Required for Windows\n",
    "            )\n",
    "            print(\"Ollama is starting...\", flush=True)\n",
    "            return ollama_process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Ollama: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# When Backend starts\n",
    "ollama_process = start_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def get_embedding_model(model_name, model_kwargs, path):\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize HuggingFaceEmbeddings with model name and kwargs\n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        cache_folder=path\n",
    "    )\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title get Ollama model\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def get_ollama_model(model):\n",
    "    llm = Ollama(model=model)\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "embedding_model_save_path = r\"E:\\CSE299\\chatbot\\llm\\baai\"\n",
    "\n",
    "\n",
    "\n",
    "embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
    "\n",
    "# Assuming get_embedding_model is defined to support a 'path' argument for saving locally\n",
    "embedding = get_embedding_model(embedding_model_name, model_kwargs, path=embedding_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "embedding_vector_db_path = r\"E:\\CSE299\\chatbot\\Embedding\\baai\\recursive\"\n",
    "\n",
    "vectorstore = Chroma(persist_directory=embedding_vector_db_path, embedding_function=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_COT_PROMPT_WITH_HISTORY = \"\"\"\n",
    "Here's the conversation so far:\n",
    "{history}\n",
    "\n",
    "If any information from the previous conversation matches the user's current question or provides useful background, feel free to incorporate it into your response. \n",
    "\n",
    "1. Identify the type of question based on the context and prior conversation:\n",
    "   - For greetings or casual questions, respond with a friendly, concise message.\n",
    "   - For straightforward information requests, provide a direct answer.\n",
    "   - For complex or multi-step questions, break down each part and answer carefully.\n",
    "\n",
    "2. Evaluate the current question:\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "\n",
    "3. Use relevant context to support accuracy and clarity in your response if applicable.\n",
    "\n",
    "Begin crafting your response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_ollama_model(\"qwen2.5:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory, ConversationBufferMemory\n",
    "\n",
    "entity_memory = ConversationEntityMemory(\n",
    "    llm=llm,\n",
    "    human_prefix=\"User\",\n",
    "    ai_prefix=\"Chatbot\",\n",
    "    memory_key=\"entities\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# Initialize conversation buffer memory\n",
    "conversation_buffer = ConversationBufferMemory(\n",
    "    human_prefix=\"User\",\n",
    "    ai_prefix=\"Chatbot\",\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following context, conversation history, and extracted entities to answer the question clearly:\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{history}\n",
    "\n",
    "Entities:\n",
    "{entities}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in simple and detailed terms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap, RunnableSequence\n",
    "\n",
    "# llm = get_ollama_model(\"gemma2:2b\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# Define a function to extract memory outputs\n",
    "def extract_memory_data(user_input):\n",
    "    history = conversation_buffer.load_memory_variables({}).get(\"history\", \"\")\n",
    "    entities = entity_memory.load_memory_variables({}).get(\"entities\", \"\")\n",
    "    context = retriever.get_relevant_documents(user_input)\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"history\": history,\n",
    "        \"entities\": entities,\n",
    "        \"question\": user_input,\n",
    "    }\n",
    "\n",
    "\n",
    "# # Build the RAG retrieval chain\n",
    "# retrieval_chain = RunnableSequence(\n",
    "#     steps=[\n",
    "#         RunnableMap(extract_memory_data),  # Prepare inputs for the chain\n",
    "#         prompt,\n",
    "#         llm,\n",
    "#         StrOutputParser(),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# for chunk in retrieval_chain.stream(\"explain newtons first law with example\"):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Terminal chat function\n",
    "def chat():\n",
    "\n",
    "    \n",
    "    print(\"Chatbot: Hello! Ask me anything. Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "          \n",
    "        # Prepare the input data\n",
    "        try:\n",
    "            input = f\"You: {user_input}\"  # Replace with your \n",
    "            print(f\"{input}\")\n",
    "            inputs = extract_memory_data(user_input)\n",
    "            # Build the retrieval chain using RunnablePassthrough\n",
    "            retrieval_chain = (\n",
    "                RunnablePassthrough()  # Pass the inputs directly through\n",
    "                | PROMPT_TEMPLATE  # Format the prompt with the inputs\n",
    "                | llm  # Invoke the model with the formatted prompt\n",
    "                | StrOutputParser()  # Parse the model's output into a string\n",
    "            )\n",
    "            \n",
    "            response = retrieval_chain.invoke(inputs)\n",
    "            print(response)\n",
    "            conversation_buffer.save_context({\"input\": user_input}, {\"output\": response})\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
